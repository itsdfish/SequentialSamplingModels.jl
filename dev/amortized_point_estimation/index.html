<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Point Estimation · SequentialSamplingModels</title><meta name="title" content="Point Estimation · SequentialSamplingModels"/><meta property="og:title" content="Point Estimation · SequentialSamplingModels"/><meta property="twitter:title" content="Point Estimation · SequentialSamplingModels"/><meta name="description" content="Documentation for SequentialSamplingModels."/><meta property="og:description" content="Documentation for SequentialSamplingModels."/><meta property="twitter:description" content="Documentation for SequentialSamplingModels."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="SequentialSamplingModels logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">SequentialSamplingModels</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Single Choice Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../ex_gaussian/">Ex-Gaussian</a></li><li><a class="tocitem" href="../shifted_lognormal/">Shifted Log Normal</a></li><li><a class="tocitem" href="../wald/">Wald</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Multi-choice Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-2-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-2-1"><span class="docs-label">Single Attribute Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../DDM/">Drift Diffusion Model (DDM)</a></li><li><a class="tocitem" href="../lca/">Leaky Competing Accumulator (LCA)</a></li><li><a class="tocitem" href="../lba/">Linear Ballistic Accumulator (LBA)</a></li><li><a class="tocitem" href="../lnr/">Log Normal Race (LNR)</a></li><li><a class="tocitem" href="../poisson_race/">Poisson Race</a></li><li><a class="tocitem" href="../rdm/">Racing Diffusion Model (RDM)</a></li><li><a class="tocitem" href="../stDDM/">Starting-time Drift Diffusion Model (stDDM)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2-2"><span class="docs-label">Multi-attribute Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../maaDDM/">Muti-attribute Attentional Drift Diffusion Model</a></li><li><a class="tocitem" href="../mdft/">Multi-attribute Decision Field Theory</a></li><li><a class="tocitem" href="../mlba/">Multi-attribute Linear Ballistic Accumulator</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Alternative Geometries</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../cddm/">Circular Drift Diffusion Model (CDDM)</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox" checked/><label class="tocitem" for="menuitem-3"><span class="docs-label">Parameter Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mode_estimation/">Mode Estimation</a></li><li><a class="tocitem" href="../turing_simple/">Simple Bayesian Model</a></li><li><a class="tocitem" href="../turing_advanced/">Advanced Model Specification</a></li><li><a class="tocitem" href="../turing_hierarchical/">Hierarchical Models</a></li><li><input class="collapse-toggle" id="menuitem-3-5" type="checkbox" checked/><label class="tocitem" for="menuitem-3-5"><span class="docs-label">Amortized Neural Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Point Estimation</a><ul class="internal"><li><a class="tocitem" href="#Full-Code"><span>Full Code</span></a></li><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Load-Packages"><span>Load Packages</span></a></li><li><a class="tocitem" href="#Define-Parameter-Sampling"><span>Define Parameter Sampling</span></a></li><li><a class="tocitem" href="#Define-Data-Simulator"><span>Define Data Simulator</span></a></li><li><a class="tocitem" href="#Define-Neural-Network-Architecture"><span>Define Neural Network Architecture</span></a></li><li><a class="tocitem" href="#Training-the-Neural-Estimator"><span>Training the Neural Estimator</span></a></li><li><a class="tocitem" href="#Assessing-Estimator-Performance"><span>Assessing Estimator Performance</span></a></li><li><a class="tocitem" href="#Visualizing-Parameter-Recovery"><span>Visualizing Parameter Recovery</span></a></li><li><a class="tocitem" href="#Using-the-Trained-Estimator"><span>Using the Trained Estimator</span></a></li><li><a class="tocitem" href="#Notes-on-Performance"><span>Notes on Performance</span></a></li><li class="toplevel"><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../amortized_bayesian_parameter_estimation/">Bayesian Parameter Estimation</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Model Comparison</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../bayes_factor/">Bayes Factors</a></li><li><a class="tocitem" href="../loo_compare/">PSIS-LOO</a></li></ul></li><li><a class="tocitem" href="../predictive_distributions/">Predictive Distributions</a></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Plotting</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../basic_plot_example/">Basic Example</a></li><li><a class="tocitem" href="../layout/">Changing the Layout</a></li><li><a class="tocitem" href="../plot_model/">Plot Model Process</a></li></ul></li><li><a class="tocitem" href="../issues/">Help and Issues</a></li><li><a class="tocitem" href="../performance_tips/">Performance Tips</a></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../developer_guide/">Developer Guide</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Parameter Estimation</a></li><li><a class="is-disabled">Amortized Neural Estimation</a></li><li class="is-active"><a href>Point Estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Point Estimation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/itsdfish/SequentialSamplingModels.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/itsdfish/SequentialSamplingModels.jl/blob/master/docs/src/amortized_point_estimation.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Parameter-Estimation"><a class="docs-heading-anchor" href="#Neural-Parameter-Estimation">Neural Parameter Estimation</a><a id="Neural-Parameter-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Parameter-Estimation" title="Permalink"></a></h1><p>Our goal is to illustrate how to use <a href="https://github.com/msainsburydale/NeuralEstimators">NeuralEstimators.jl</a> to perform neural parameter estimation of the Leaky Competing Accumulator (LCA; Usher &amp; McClelland, 2001). In our example below, we estimate the mean of the posterior distributions. Neural parameter estimation uses neural networks to perform parameter estimation by learning the mapping between simulated data and model parameters (for a detailed review, see Zammit-Mangion et al., 2024). Neural parameter estimation is particularly useful for models with computationally intractable likelihoods, such as the LCA. Many neural estimation teachniques are amortized, meaning one incurs a large initial computational cost to train the neural estimator, but estimating the parameters with the trained network is fast and computationally efficient. One benefit of amortized approaches is that the trained neural estimator can be saved and reused across multiple datasets, or used for computationally intensive parameter recovery simulations to understand the quality of parameter estimates under ideal conditions. </p><h2 id="Full-Code"><a class="docs-heading-anchor" href="#Full-Code">Full Code</a><a id="Full-Code-1"></a><a class="docs-heading-anchor-permalink" href="#Full-Code" title="Permalink"></a></h2><p>For those who are interested only in the code, you can click on the ▶ icon below to reveal a full copy-and-pastable version of the example.</p><details>
<summary><b>Full Code</b></summary><pre><code class="language-julia hljs">using Distributions
using Flux
using NeuralEstimators
using Plots
using SequentialSamplingModels
Random.seed!(123)

# Function to sample parameters from priors
function sample(K::Integer)
    ν1 = rand(Gamma(2, 1/1.2f0), K)  # Drift rate 1
    ν2 = rand(Gamma(2, 1/1.2f0), K)  # Drift rate 2
    α = rand(Gamma(10, 1/6f0), K)    # Threshold
    β = rand(Beta(1, 5f0), K)        # Lateral inhibition
    λ = rand(Beta(1, 5f0), K)        # Leak rate
    τ = rand(Gamma(1.5, 1/5.0f0), K) # Non-decision time
    
    # Stack parameters into a matrix (d×K)
    θ = vcat(ν1&#39;, ν2&#39;, α&#39;, β&#39;, λ&#39;, τ&#39;)
    
    return θ
end

# Function to simulate data from the LCA model
function simulate(θ, n_trials_per_param)
    # Simulate data for each parameter vector
    simulated_data = map(eachcol(θ)) do param
        # Extract parameters for this model
        ν1, ν2, α, β, λ, τ = param
        ν = [ν1, ν2]   # Two-choice LCA
        
        # Create LCA model with SSM
        model = LCA(; ν, α, β, λ, τ)
        
        # Generate choices and reaction times
        choices, rts = rand(model, n_trials_per_param)
        
        # Return as a transpose matrix where each column is a trial 
        return Float32.([choices rts]&#39;)
    end
    return simulated_data
end

# Create neural network architecture for parameter recovery
function create_neural_estimator(;
    ν_bounds = (0.1, 6.0),
    α_bounds = (0.3, 4.5),
    β_bounds = (0.0, 0.8),
    λ_bounds = (0.0, 0.8),
    τ_bounds = (0.100, 2.0)
)
    # Unpack defined parameter Bounds
    ν_min, ν_max = ν_bounds              # Drift rates
    α_min, α_max = α_bounds              # Threshold
    β_min, β_max = β_bounds              # Lateral inhibition
    λ_min, λ_max = λ_bounds              # Leak rate
    τ_min, τ_max = τ_bounds              # Non-decision time

    # Input dimension: 2 (choice and RT for each trial)
    n = 2
    # Output dimension: 6 parameters
    d = 6  # ν[1], ν[2], α, β, λ, τ
    # Width of hidden layers
    w = 128
    
    # Inner network - processes each trial independently
    ψ = Chain(
        Dense(n, w, relu),
        Dense(w, w, relu),
        Dense(w, w, relu)
    )
    
    # Final layer with parameter constraints
    final_layer = Parallel(
        vcat,
        Dense(w, 1, x -&gt; ν_min + (ν_max - ν_min) * σ(x)),     # ν1
        Dense(w, 1, x -&gt; ν_min + (ν_max - ν_min) * σ(x)),     # ν2
        Dense(w, 1, x -&gt; α_min + (α_max - α_min) * σ(x)),     # α
        Dense(w, 1, x -&gt; β_min + (β_max - β_min) * σ(x)),     # β
        Dense(w, 1, x -&gt; λ_min + (λ_max - λ_min) * σ(x)),     # λ
        Dense(w, 1, x -&gt; τ_min + (τ_max - τ_min) * σ(x))      # τ
    )
    
    # Outer network - maps aggregated features to parameters
    ϕ = Chain(
        Dense(w, w, relu),
        Dense(w, w, relu),
        final_layer
    )
    
    # Combine into a DeepSet
    network = DeepSet(ψ, ϕ)
    
    # Initialize neural Bayes estimator
    estimator = PointEstimator(network)
    
    return estimator
end

# Create the neural estimator
estimator = create_neural_estimator()

# Train network
trained_estimator = train(
    estimator,
    sample,                     # Parameter sampler function
    simulate,                   # Data simulator function
    m = 100,                    # Number of trials per parameter vector
    K = 10000,                  # Number of training parameter vectors
    K_val = 2000,               # Number of validation parameter vectors
    loss = Flux.mae,            # Mean absolute error loss
    epochs = 60,                # Number of training epochs
    epochs_per_Z_refresh = 1,   # Refresh data every epoch
    epochs_per_θ_refresh = 5,   # Refresh parameters every 5 epochs
    batchsize = 16,             # Batch size for training
    verbose = true
)

# Generate test data
n_test = 500
θ_test = sample(n_test)
Z_test = simulate(θ_test, 500)

# Assess the estimator
parameter_names = [&quot;ν1&quot;, &quot;ν2&quot;, &quot;α&quot;, &quot;β&quot;, &quot;λ&quot;, &quot;τ&quot;]
assessment = assess(
    trained_estimator, 
    θ_test, 
    Z_test; 
    parameter_names = parameter_names
)

# Calculate performance metrics
bias_results = bias(assessment)
rmse_results = rmse(assessment)
println(&quot;Bias: &quot;, bias_results)
println(&quot;RMSE: &quot;, rmse_results)

# Extract data from assessment
df = assessment.df

# Create recovery plots for each parameter
params = unique(df.parameter)
p_plots = []

for param in params
    param_data = filter(row -&gt; row.parameter == param, df)
    
    # Calculate correlation coefficient
    truth = param_data.truth
    estimate = param_data.estimate
    correlation = cor(truth, estimate)
    
    # Create plot
    p = scatter(
        truth, 
        estimate,
        xlabel=&quot;Ground Truth&quot;,
        ylabel=&quot;Estimated&quot;,
        title=param,
        legend=false
    )
    
    # Add diagonal reference line
    plot!(p, [minimum(truth), maximum(truth)], 
          [minimum(truth), maximum(truth)], 
          line=:dash, color=:black)
    
    # Get current axis limits after plot is created
    x_min, x_max = xlims(p)
    y_min, y_max = ylims(p)
    
    # Position text at the top-left corner of the plot
    annotate!(p, x_min + 0.1, y_max, text(&quot;R = $(round(correlation, digits=3))&quot;, :left, 10))
    
    push!(p_plots, p)
end

# Combine plots
p_combined = plot(p_plots..., layout=(3,2), size=(800, 600))
display(p_combined)

# Generate &quot;observed&quot; data
ν = [2.5, 2.0]
α = 1.5
β = 0.2
λ = 0.1
τ = 0.3

# Create model and generate data
true_model = LCA(; ν, α, β, λ, τ)
observed_choices, observed_rts = rand(true_model, 100)

# Format the data
observed_data = Float32.([observed_choices observed_rts]&#39;)

# Recover parameters
recovered_params = NeuralEstimators.estimate(trained_estimator, [observed_data])

# Compare true and recovered parameters
println(&quot;True parameters: &quot;, [ν[1], ν[2], α, β, λ, τ])
println(&quot;Recovered parameters: &quot;, recovered_params)</code></pre></details><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>We&#39;ll estimate parameters of the LCA model, which is particularly challenging due to its complex dynamics, where parameters like leak rate (λ) and lateral inhibition (β) can be difficult to recover (Miletić et al., 2017). This example draws from a more in-depth case that highlights many of the steps one ought to consider when utilizing amortized inference for cognitive modeling; see <a href="https://bayesflow.org/stable-legacy/_examples/LCA_Model_Posterior_Estimation.html">Principled Amortized Bayesian Workflow for Cognitive Modeling</a>.</p><h2 id="Load-Packages"><a class="docs-heading-anchor" href="#Load-Packages">Load Packages</a><a id="Load-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using Distributions
using Flux
using NeuralEstimators
using Plots
using SequentialSamplingModels
Random.seed!(123)</code></pre><h2 id="Define-Parameter-Sampling"><a class="docs-heading-anchor" href="#Define-Parameter-Sampling">Define Parameter Sampling</a><a id="Define-Parameter-Sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Define-Parameter-Sampling" title="Permalink"></a></h2><p>Unlike traditional Bayesian inference methods, neural parameter estimation requires us to define two functions so that the neural network can learn the mapping between simulated data and parameters. One function samples parameters from a prior distribution, and the other generates simulated data based on a sampled parameter vector. While traditional methods like MCMC also sample from the prior, those samples are used directly during inference rather than to create a separate training dataset. </p><p><img src="../assets/npe_example.png" alt/></p><p><em>Schematic of neural parameter estimation. Once trained, the neural network provides a direct mapping from observed data (Z) to parameter estimates (θ̂), enabling rapid inference without the computational burden of traditional methods.</em></p><p>In neural parameter estimation, we use the prior to sample a wide range of parameters and simulate corresponding data, which we then use to train a model (e.g., a neural network) to approximate a point estimate or the posterior. We use the following function to sample a range of parameters for training:</p><pre><code class="language-julia hljs"># Function to sample parameters from priors
function sample(K::Integer)
    ν1 = rand(Gamma(2, 1/1.2f0), K)  # Drift rate 1
    ν2 = rand(Gamma(2, 1/1.2f0), K)  # Drift rate 2
    α = rand(Gamma(10, 1/6f0), K)    # Threshold
    β = rand(Beta(1, 5f0), K)        # Lateral inhibition
    λ = rand(Beta(1, 5f0), K)        # Leak rate
    τ = rand(Gamma(1.5, 1/5.0f0), K) # Non-decision time
    # Stack parameters into a matrix (d×K)
    θ = vcat(ν1&#39;, ν2&#39;, α&#39;, β&#39;, λ&#39;, τ&#39;)
    return θ
end</code></pre><h2 id="Define-Data-Simulator"><a class="docs-heading-anchor" href="#Define-Data-Simulator">Define Data Simulator</a><a id="Define-Data-Simulator-1"></a><a class="docs-heading-anchor-permalink" href="#Define-Data-Simulator" title="Permalink"></a></h2><p>Neural estimators learn the mapping from data to parameters through simulation. Here we define a function to simulate LCA model data. To do so we will use the <a href="https://itsdfish.github.io/SequentialSamplingModels.jl/dev/lca/">LCA</a>.</p><pre><code class="language-julia hljs"># Function to simulate data from the LCA model
function simulate(θ, n_trials_per_param)
    # Simulate data for each parameter vector
    simulated_data = map(eachcol(θ)) do param
        # Extract parameters for this model
        ν1, ν2, α, β, λ, τ = param
        ν = [ν1, ν2]   # Two-choice LCA
        # Create LCA model with SSM
        model = LCA(; ν, α, β, λ, τ)
        # Generate choices and reaction times
        choices, rts = rand(model, n_trials_per_param)
        # Return as a transpose matrix where each column is a trial 
        return Float32.([choices rts]&#39;)
    end
    return simulated_data
end</code></pre><h2 id="Define-Neural-Network-Architecture"><a class="docs-heading-anchor" href="#Define-Neural-Network-Architecture">Define Neural Network Architecture</a><a id="Define-Neural-Network-Architecture-1"></a><a class="docs-heading-anchor-permalink" href="#Define-Neural-Network-Architecture" title="Permalink"></a></h2><p>For LCA parameter recovery, we use a DeepSet architecture which respects the permutation invariance of trial data. For more details on the method <a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/architectures/#NeuralEstimators.DeepSet">see NeuralEstimators.jl documentation</a>. To construct the network architecture we will use the Flux.jl package.</p><pre><code class="language-julia hljs"># Create neural network architecture for parameter recovery
function create_neural_estimator(;
    ν_bounds = (0.1, 6.0),
    α_bounds = (0.3, 4.5),
    β_bounds = (0.0, 0.8),
    λ_bounds = (0.0, 0.8),
    τ_bounds = (0.100, 2.0)
)
    # Unpack defined parameter Bounds
    ν_min, ν_max = ν_bounds              # Drift rates
    α_min, α_max = α_bounds              # Threshold
    β_min, β_max = β_bounds              # Lateral inhibition
    λ_min, λ_max = λ_bounds              # Leak rate
    τ_min, τ_max = τ_bounds              # Non-decision time

    # Input dimension: 2 (choice and RT for each trial)
    n = 2
    # Output dimension: 6 parameters
    d = 6  # ν[1], ν[2], α, β, λ, τ
    # Width of hidden layers
    w = 128
    
    # Inner network - processes each trial independently
    ψ = Chain(
        Dense(n, w, relu),
        Dense(w, w, relu),
        Dense(w, w, relu)
    )
    
    # Final layer with parameter constraints
    final_layer = Parallel(
        vcat,
        Dense(w, 1, x -&gt; ν_min + (ν_max - ν_min) * σ(x)),     # ν1
        Dense(w, 1, x -&gt; ν_min + (ν_max - ν_min) * σ(x)),     # ν2
        Dense(w, 1, x -&gt; α_min + (α_max - α_min) * σ(x)),     # α
        Dense(w, 1, x -&gt; β_min + (β_max - β_min) * σ(x)),     # β
        Dense(w, 1, x -&gt; λ_min + (λ_max - λ_min) * σ(x)),     # λ
        Dense(w, 1, x -&gt; τ_min + (τ_max - τ_min) * σ(x))      # τ
    )
    
    # Outer network - maps aggregated features to parameters
    ϕ = Chain(
        Dense(w, w, relu),
        Dense(w, w, relu),
        final_layer
    )
    
    # Combine into a DeepSet
    network = DeepSet(ψ, ϕ)
    
    # Initialize neural Bayes estimator
    estimator = PointEstimator(network)
    
    return estimator
end</code></pre><p>The result of our constructed neural network is a point estimator that corresponds to a Bayes estimator, which is a functional of the posterior distribution. Under the specified loss function, this point estimate corresponds to the posterior mean. For details on the theoretical foundations of neural Bayes estimators, see Sainsbury-Dale et al. (2024).</p><h2 id="Training-the-Neural-Estimator"><a class="docs-heading-anchor" href="#Training-the-Neural-Estimator">Training the Neural Estimator</a><a id="Training-the-Neural-Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-Neural-Estimator" title="Permalink"></a></h2><p>Neural estimators, like all deep learning methods, require a training phase during which they learn the mapping from data to parameters. Here, we train the estimator by simulating data on the fly: the sampler provides new parameter vectors from the prior, and the simulator generates corresponding data conditional on those parameters. Since we use online training and the network never sees the same simulated dataset twice, overfitting is less likely. For more details on training, see the API for arguments <a href="https://msainsburydale.github.io/NeuralEstimators.jl/dev/API/core/#Training">here</a>.</p><pre><code class="language-julia hljs"># Create the neural estimator
estimator = create_neural_estimator()

# Train network
trained_estimator = train(
    estimator,
    sample,                     # Parameter sampler function
    simulate,                   # Data simulator function
    m = 100,                    # Number of trials per parameter vector
    K = 10000,                  # Number of training parameter vectors
    K_val = 2000,               # Number of validation parameter vectors
    loss = Flux.mae,            # Mean absolute error loss
    epochs = 60,                # Number of training epochs
    epochs_per_Z_refresh = 1,   # Refresh data every epoch
    epochs_per_θ_refresh = 5,   # Refresh parameters every 5 epochs
    batchsize = 16,             # Batch size for training
    verbose = true
)</code></pre><h2 id="Assessing-Estimator-Performance"><a class="docs-heading-anchor" href="#Assessing-Estimator-Performance">Assessing Estimator Performance</a><a id="Assessing-Estimator-Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Assessing-Estimator-Performance" title="Permalink"></a></h2><p>We can assess the performance of our trained estimator on held-out test data:</p><pre><code class="language-julia hljs"># Generate test data
n_test = 500
θ_test = sample(n_test)
Z_test = simulate(θ_test, 500)

# Assess the estimator
parameter_names = [&quot;ν1&quot;, &quot;ν2&quot;, &quot;α&quot;, &quot;β&quot;, &quot;λ&quot;, &quot;τ&quot;]
assessment = assess(
    trained_estimator, 
    θ_test, 
    Z_test; 
    parameter_names = parameter_names
)

# Calculate performance metrics
bias_results = bias(assessment)
rmse_results = rmse(assessment)
println(&quot;Bias: &quot;, bias_results)
println(&quot;RMSE: &quot;, rmse_results)</code></pre><h2 id="Visualizing-Parameter-Recovery"><a class="docs-heading-anchor" href="#Visualizing-Parameter-Recovery">Visualizing Parameter Recovery</a><a id="Visualizing-Parameter-Recovery-1"></a><a class="docs-heading-anchor-permalink" href="#Visualizing-Parameter-Recovery" title="Permalink"></a></h2><p>A key advantage of neural estimation is the ability to quickly conduct inference after training. For example, we can visualize the recovery of parameters. While NeuralEstimators provides built-in visualization capabilities through the <a href="https://github.com/MakieOrg/AlgebraOfGraphics.jl">AlgebraOfGraphics.jl</a>, we will demonstrate custom plotting below:</p><pre><code class="language-julia hljs"># Extract data from assessment
df = assessment.df

# Create recovery plots for each parameter
params = unique(df.parameter)
p_plots = []

for param in params
    param_data = filter(row -&gt; row.parameter == param, df)
    
    # Calculate correlation coefficient
    truth = param_data.truth
    estimate = param_data.estimate
    correlation = cor(truth, estimate)
    
    # Create plot
    p = scatter(
        truth, 
        estimate,
        xlabel=&quot;Ground Truth&quot;,
        ylabel=&quot;Estimated&quot;,
        title=param,
        legend=false
    )
    
    # Add diagonal reference line
    plot!(p, [minimum(truth), maximum(truth)], 
          [minimum(truth), maximum(truth)], 
          line=:dash, color=:black)
    
    # Get current axis limits after plot is created
    x_min, x_max = xlims(p)
    y_min, y_max = ylims(p)
    
    # Position text at the top-left corner of the plot
    annotate!(p, x_min + 0.1, y_max, text(&quot;R = $(round(correlation, digits=3))&quot;, :left, 10))
    
    push!(p_plots, p)
end

# Combine plots
p_combined = plot(p_plots..., layout=(3,2), size=(800, 600))
display(p_combined)</code></pre><p><img src="../assets/lca_amorized_recovery.png" alt/></p><h2 id="Using-the-Trained-Estimator"><a class="docs-heading-anchor" href="#Using-the-Trained-Estimator">Using the Trained Estimator</a><a id="Using-the-Trained-Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-Trained-Estimator" title="Permalink"></a></h2><p>Once trained, the estimator can instantly recover parameters from new data via a forward pass:</p><pre><code class="language-julia hljs"># Generate &quot;observed&quot; data
ν = [2.5, 2.0]
α = 1.5
β = 0.2
λ = 0.1
τ = 0.3

# Create model and generate data
true_model = LCA(; ν, α, β, λ, τ)
observed_choices, observed_rts = rand(true_model, 100)

# Format the data
observed_data = Float32.([observed_choices observed_rts]&#39;)

# Recover parameters
recovered_params = NeuralEstimators.estimate(trained_estimator, [observed_data])

# Compare true and recovered parameters
println(&quot;True parameters: &quot;, [ν[1], ν[2], α, β, λ, τ])
println(&quot;Recovered parameters: &quot;, recovered_params)</code></pre><h2 id="Notes-on-Performance"><a class="docs-heading-anchor" href="#Notes-on-Performance">Notes on Performance</a><a id="Notes-on-Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Notes-on-Performance" title="Permalink"></a></h2><p>Neural estimators are particularly effective for models with computationally intractable likelihoods like the LCA model. However, certain parameters (particularly β and λ) can be difficult to recover, even with advanced neural network architectures (Miletić et al., 2017). This is a property of the LCA model rather than a limitation of the estimation technique. </p><p>Additional details can be found in the <a href="https://github.com/msainsburydale/NeuralEstimators">NeuralEstimators.jl documentation</a>.</p><h1 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h1><p>Miletić, S., Turner, B. M., Forstmann, B. U., &amp; van Maanen, L. (2017). Parameter recovery for the leaky competing accumulator model. Journal of Mathematical Psychology, 76, 25-50.</p><p>Sainsbury-Dale, Matthew, Andrew Zammit-Mangion, and Raphaël Huser. &quot;Likelihood-free parameter estimation with neural Bayes estimators.&quot; The American Statistician 78.1 (2024): 1-14.</p><p>Radev, S. T., Schmitt, M., Schumacher, L., Elsemüller, L., Pratz, V., Schälte, Y., ... &amp; Bürkner, P. C. (2023). BayesFlow: Amortized Bayesian workflows with neural networks. arXiv preprint arXiv:2306.16015.</p><p>Usher, M., &amp; McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108 3, 550–592. https://doi.org/10.1037/0033-295X.108.3.550</p><p>Zammit-Mangion, Andrew, Matthew Sainsbury-Dale, and Raphaël Huser. &quot;Neural methods for amortized inference.&quot; Annual Review of Statistics and Its Application 12 (2024).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../turing_hierarchical/">« Hierarchical Models</a><a class="docs-footer-nextpage" href="../amortized_bayesian_parameter_estimation/">Bayesian Parameter Estimation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 31 December 2025 17:03">Wednesday 31 December 2025</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
